{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "“lab02_Embeddings_solutions.ipynb”的副本",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Krystal-Jing/NLP_Course_Materials/blob/master/%E2%80%9Clab02_Embeddings_solutions_ipynb%E2%80%9D%E7%9A%84%E5%89%AF%E6%9C%AC.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ArV7fHIFHMEY",
        "colab_type": "text"
      },
      "source": [
        "# Semantic word representations\n",
        "\n",
        "Semantic Word Representations are representations that are learned to capture the 'meaning' of a word based on the contexts it occurs in. These are low-dimensional vectors that contain some semantic properties. In this notebook we are going to build state-of-the art approaches to obtain semantic word representations using the `word2vec` modelling approach. We will use these vectors in some tasks to understand the utility of these representations. Later we will also learn how to use such already pre-trained representations (BERT) in those tasks.\n",
        "\n",
        "We begin by loading some of the libraries that are necessary for building our model. We are using [pytorch](https://pytorch.org/), an open source deep learning platform, as our backbone library in the course."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5qS8y5_Ewv6n",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "from torch.autograd import Variable\n",
        "import numpy as np\n",
        "import torch.nn.functional as F\n",
        "from scipy.spatial.distance import euclidean, cosine\n",
        "from tqdm import tqdm \n",
        "import codecs\n",
        "from sklearn.metrics.pairwise import cosine_distances\n",
        "import nltk\n",
        "nltk.download('punkt')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zr6d36L4OX22",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "corpus = [\n",
        "    'he is a king',\n",
        "    'she is a queen',\n",
        "    'he is a Man',\n",
        "    'she Is a woman',\n",
        "    'london is, the capital of England',\n",
        "    'Berlin is ... the capital of germany',\n",
        "    'paris is the capital of france.',\n",
        "    'He will eat cake, pie, and/or brownies',\n",
        "    \"she didn't like the brownies\"\n",
        "]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FE8B9-L8U0aZ",
        "colab_type": "text"
      },
      "source": [
        "We will reuse the pre-processing and vocabulary methods we have created in the previous lab."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "abMSnMwJx8bu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import string\n",
        "import re\n",
        "from nltk import word_tokenize\n",
        "\n",
        "def preprocess_corpus(corpus):\n",
        "  tokenized_corpus = [] \n",
        "  for sentence in corpus:\n",
        "    sentence = \" \".join(word_tokenize(sentence))\n",
        "    tokenized_sentence = []\n",
        "    for token in sentence.split(' '):\n",
        "      token = token.lower()\n",
        "      tokenized_sentence.append(token)\n",
        "    tokenized_corpus.append(tokenized_sentence)\n",
        "  return tokenized_corpus\n",
        "\n",
        "def get_vocabulary(tokenized_corpus):\n",
        "  vocabulary = []\n",
        "  for sentence in tokenized_corpus:\n",
        "    for token in sentence:\n",
        "        vocabulary.append(token)\n",
        "  return set(vocabulary)\n",
        "\n",
        "tokenized_corpus = preprocess_corpus(corpus)\n",
        "print(tokenized_corpus)\n",
        "\n",
        "vocabulary = get_vocabulary(tokenized_corpus)\n",
        "vocabulary_size = len(vocabulary)\n",
        "print(vocabulary_size)\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0hPlpqVFYpfL",
        "colab_type": "text"
      },
      "source": [
        "##Helper functions\n",
        "These are some of other common helper functions that are used for NLP models:\n",
        "\n",
        "`word2idx`: Maintains a dictionary of word and the corresponding index\n",
        "\n",
        "`idx2word`: Maintains a mapping from index to word\n",
        "\n",
        "Print the word2idx and idx2word, we will be using these in future exercises."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UQ5IL1e1GVn6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "word2idx = {w: idx for (idx, w) in enumerate(vocabulary)}\n",
        "idx2word = {idx: w for (idx, w) in enumerate(vocabulary)}\n",
        "\n",
        "print(word2idx)\n",
        "print(idx2word)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cyKCfY6QbO0r",
        "colab_type": "text"
      },
      "source": [
        "## Look-up table\n",
        "\n",
        "This is a table that maps from an index to a one hot vector. It is a categorical variable binary representation where each row has one feature with a value of 1, and the other features with value 0. 1 in a particular column will tell the correct category for each example.\n",
        "\n",
        "**Q. Print one-hot vectors corresponding to the words 'the', 'he' and ''england'**\n",
        "\n",
        "*A. See code below*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9bWQ3zKYKlQU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def look_up_table(word_idx):\n",
        "    x = torch.zeros(vocabulary_size).float()\n",
        "    x[word_idx] = 1.0\n",
        "    return x\n",
        "  \n",
        "# This is a one hot representation\n",
        "\n",
        "# Q. try printing it for word_idx = 1\n",
        "\n",
        "word_idx = word2idx['he']\n",
        "print(look_up_table(word_idx))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hve1MProNBvp",
        "colab_type": "text"
      },
      "source": [
        "##Extracting contexts and the focus word\n",
        "\n",
        "We will be building the **skip-gram** model. Given a corpus this model tries to use the current word (focus word) to predict its neighbors (its context)\n",
        "\n",
        "We first begin by obtaining the set of contexts and focus words.\n",
        "\n",
        "Let us say we have a sentence (represented as vocabulary indicies): [0, 2, 3, 6, 7].\n",
        "For every word in the sentence, we want to get the words which are window_size around it.\n",
        "So if window_size==2, for the word '0', we obtain: [[0, 2], [0, 3]]\n",
        "For the word '2', we obtain: [[2, 0], [2, 3], [2, 6]]\n",
        "For the word '3', we obtain: [[3, 0], [3, 2], [3, 6], [3, 7]]\n",
        "\n",
        "**Q. Print some of the index pairs and trace them back to their words.**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J-xp_gKvObfw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "window_size = 2\n",
        "idx_pairs = []\n",
        "\n",
        "# variables of interest: \n",
        "#   center_word_pos: center word position\n",
        "#   context_word_pos: context_word_position\n",
        "#   add sentence length as a constraint\n",
        "\n",
        "for sentence in tokenized_corpus:\n",
        "    indices = [word2idx[word] for word in sentence]\n",
        "    \n",
        "    for center_word_pos in range(len(indices)):\n",
        "        \n",
        "        for w in range(-window_size, window_size + 1):\n",
        "            context_word_pos = center_word_pos + w\n",
        "            \n",
        "            if context_word_pos < 0 or context_word_pos >= len(indices) or center_word_pos == context_word_pos:\n",
        "                continue\n",
        "                \n",
        "            context_word_idx = indices[context_word_pos]\n",
        "            idx_pairs.append((indices[center_word_pos], context_word_idx))\n",
        "\n",
        "idx_pairs = np.array(idx_pairs) # it will be useful to have this as numpy array\n",
        "\n",
        "print(idx_pairs)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bKKEpzKdkjBt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# We'll sample 5 elements at random and trace these back to their word pairs\n",
        "from random import sample\n",
        "\n",
        "random_pairs = sample(list(idx_pairs), 5)\n",
        "print(random_pairs)\n",
        "\n",
        "tokens_from_idx_pairs = []\n",
        "for random_pair in random_pairs:\n",
        "    focus_word_idx, context_word_idx = random_pair[0], random_pair[1]\n",
        "    focus_word = idx2word[focus_word_idx]\n",
        "    context_word = idx2word[context_word_idx]\n",
        "    tokens_from_idx_pairs.append([focus_word, context_word])\n",
        "\n",
        "print()\n",
        "print(tokens_from_idx_pairs)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xGg6CtALe8Va",
        "colab_type": "text"
      },
      "source": [
        "## Parameters and hyperparameters¶\n",
        "For our toy task, let us set the embedding dimensions to 5. Let us run the algorithm for 10 epochs (number of times the training algorithm sees all the training data). Let us choose the learning rate of 0.001. We have two parameter matrices $W_1$ and $W_2$ - the embedding matrix and the weight matrix.\n",
        "\n",
        "**Q. What are the dimensionalities of $W_1$ and $W_2$?** \n",
        "\n",
        "*A. shape(W1) = [embedding_dims x vocabulary_size]\n",
        "shape(W2) = [vocabulary_size x embedding_dims]*\n",
        "\n",
        " \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4LWut1gtXGQN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Hyperparameters:\n",
        "embedding_dims = 5\n",
        "num_epochs = 100\n",
        "learning_rate = 0.001\n",
        "\n",
        "# The two weight matrices:\n",
        "W1 = torch.randn(embedding_dims, vocabulary_size, requires_grad=True)\n",
        "W2 = torch.randn(vocabulary_size, embedding_dims, requires_grad=True)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PcAI5BtaQMFh",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "##Training the model\n",
        "\n",
        "In the code below, we are going to compute the log probability of the correct context (target) given the word.\n",
        "\n",
        "**Q. Fill in the gaps.**\n",
        " \n",
        "**Q. Print the loss and see if the loss goes down.**\n",
        "\n",
        "\n",
        "**Q. What is the meaning of the negative log-likelihood loss (NLL) ?**\n",
        "\n",
        "The better the prediction the lower the NLL loss. Minimising the negative likelihood is the same as maximising the average log probability for the context window. We do it to be consistent with the rule of loss functions. Log is used for numerical stability. Likelihood refers to the chances that our outputs produce the target distribution.\n",
        "\n",
        "**Q. Which of the weight matrices will be extracted as word vectors ?**\n",
        "\n",
        "A. W1"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wrtOwTUsyArb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        " for epoch in tqdm(range(num_epochs)):\n",
        "  \n",
        "    loss_val = 0\n",
        "    \n",
        "    for data, target in idx_pairs:\n",
        "      \n",
        "        x = torch.Tensor(look_up_table(data))\n",
        "\n",
        "        # Q. what would y_true be? \n",
        "        y_true = torch.Tensor([target]).long()\n",
        "\n",
        "        # A. [index] of the target word\n",
        "        # \n",
        "        z1 = torch.matmul(W1, x) \n",
        "        \n",
        "        z2 = torch.matmul(W2, z1)\n",
        "        # Q. what is the above operation?\n",
        "        # A. matrix multiplication\n",
        " \n",
        "        # Let us obtain prediction over the vocabulary\n",
        "        log_softmax = F.log_softmax(z2, dim=0)\n",
        "                \n",
        "        loss = F.nll_loss(log_softmax.view(1,-1), y_true)\n",
        "        loss_val += loss.item()\n",
        "        \n",
        "        # propagate the error\n",
        "        loss.backward()\n",
        "        \n",
        "        # gradient descent\n",
        "        W1.data -= learning_rate * W1.grad.data\n",
        "        W2.data -= learning_rate * W2.grad.data\n",
        "\n",
        "        # zero out gradient accumulation\n",
        "        W1.grad.data.zero_()\n",
        "        W2.grad.data.zero_()\n",
        "\n",
        "    print(f'\\nFinal epoch loss: {loss_val/len(idx_pairs)}')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nwY-k6x0xIkl",
        "colab_type": "text"
      },
      "source": [
        "**Q. Given that we are interested in distributed representations, what is the major bottleneck in our setup? Is it the dimensionality of the representations? Is it the learning rate? Is it the corpus?**\n",
        "\n",
        "*A. It is both the dimensionality and the corpus. We have a only few words and contexts it would be difficult to capture distributional contexts. As we increase the words or expand the corpus we would have to expand our dimensions.*\n",
        "\n",
        "**Q. What hyperparameters would you tune to improve the representations?**\n",
        "\n",
        "*A. Decreasing the learning rate (this will be discussed more in further lectures)*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q4R6Y6by1dRW",
        "colab_type": "text"
      },
      "source": [
        "## Using word embeddings\n",
        "\n",
        "One of the simplest ways of exploiting word representations is to find similar words. There are many ways of measuring the semantic similarity between two words. As we are using word representations which are vectors in the euclidean space, distance metrics defined in the euclidean space are the most popular choice. This is because words that share common contexts in the corpus are located in close proximity to one another in the euclidean space. One such metric is the eucldeian distance.\n",
        "\n",
        "**Q. What is the euclidean distance between 'the' and 'a' (in the sample corpus and the new corpus)?**\n",
        "\n",
        "*A. See solution in code.*\n",
        "\n",
        "**Q. What other distance metrics can we use for two vectors?**\n",
        "\n",
        "A. Cosine distance is the most frequently used distance metric for high-dimensional space. Many other distance metrics exist and high-level overview can be found in [this blog post](https://towardsdatascience.com/importance-of-distance-metrics-in-machine-learning-modelling-e51395ffe60d)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9eP2s5MyyrNO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Let us get two vectors from the trained model\n",
        "\n",
        "x = torch.Tensor(look_up_table(0))\n",
        "x_emb = torch.matmul(W1, x).detach().numpy()\n",
        "y = torch.Tensor(look_up_table(1))\n",
        "y_emb = torch.matmul(W1, y).detach().numpy()\n",
        "\n",
        "# let us print the euclidean distance\n",
        "print(euclidean(x_emb, y_emb))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1r-bo2nwzIbI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "vector_the = look_up_table(word2idx[\"the\"])\n",
        "vector_a = look_up_table(word2idx[\"a\"])\n",
        "print(euclidean(vector_the, vector_a))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "06pjL8-BeD47",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "# Pre-trained representations\n",
        "\n",
        "We have seen from the above that word embeddings are learned in an unsupervised manner, i.e., we don't have any labelled data. These representations can be used to `bootstrap' models in NLP. There are many word representation inducing algorithms : word2vec, GloVe, Fasttext are some of the popular choices. There are differences in the algorithms but they are all based on the distributional hypothesis (a word could be defined by its distribution in language use).\n",
        "\n",
        "We will now use one of these pre-trained representations: [GloVe](https://nlp.stanford.edu/projects/glove/).\n",
        "\n",
        "**Q. What is the dimensionality of the representations below?**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lOVLlT99el3b",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!wget http://nlp.stanford.edu/data/glove.6B.zip\n",
        "!unzip glove.6B.zip"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cl4liM2HeaGv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "w2i = [] # word2index\n",
        "i2w = [] # index2word\n",
        "wvecs = [] # word vectors\n",
        "\n",
        "# this is a large file, it will take a while to load in the memory!\n",
        "with codecs.open('glove.6B.50d.txt', 'r','utf-8') as f: \n",
        "  index = 0\n",
        "  for line in tqdm(f.readlines()):\n",
        "    # Ignore the first line - first line typically contains vocab, dimensionality\n",
        "    if len(line.strip().split()) > 3:\n",
        "      \n",
        "      (word, vec) = (line.strip().split()[0], \n",
        "                     list(map(float,line.strip().split()[1:]))) \n",
        "      \n",
        "      wvecs.append(vec)\n",
        "      w2i.append((word, index))\n",
        "      i2w.append((index, word))\n",
        "      index += 1\n",
        "\n",
        "w2i = dict(w2i)\n",
        "i2w = dict(i2w)\n",
        "wvecs = np.array(wvecs)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9ENXpdI0hTZg",
        "colab_type": "text"
      },
      "source": [
        "For the following experiments, we recommend using `wvecs` - the pre-trained representations."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MbJ0ZVWjhbA2",
        "colab_type": "text"
      },
      "source": [
        "## Evaluating word representation models\n",
        " \n",
        "Evaluation of word representations is done in specific subtasks.\n",
        "\n",
        "### Word Similarity\n",
        "The first task we consider is evaluating if the representations are good at computing if two words are similar in meaning. \n",
        "\n",
        "### Exploring Analogies\n",
        "The second task we consider is completing analogies. We are given an incomplete analogy of the form:\n",
        "\n",
        "$a : b : : c :~?$\n",
        "\n",
        "We then identify the word vector which maximises the cosine similarity. Ideally, we want $\\phi(b) - \\phi(a) = \\phi(d) - \\phi(c)$ where $\\phi(.)$ is the word vector. For example, london $-$ england = paris $-$ france .\n",
        "\n",
        "\n",
        "Use the examples below to answer the following questions:\n",
        "\n",
        "**Q. When does the analogy task it fail?**\n",
        "\n",
        "*for more rare words, for grammatical relations*\n",
        "\n",
        "**Q. What are the possible reasons for failure?**\n",
        "\n",
        "*word is rare, context more ambiguos. For more explanations, refer to this [blog](https://kawine.github.io/blog/nlp/2019/06/21/word-analogies.html*\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "92fJ1uW5k5ZR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def cosine_distance(u, v):\n",
        "    distance = 0.0\n",
        "    dot = np.dot(u,v)\n",
        "    norm_u = np.sqrt(np.sum(u**2))\n",
        "    norm_v = np.sqrt(np.sum(v**2))\n",
        "    distance = dot/(norm_u)/norm_v\n",
        "    return distance\n",
        "  \n",
        " \n",
        "def find_analogy(word_a, word_b, word_c, word_vectors, word2index):\n",
        "    word_a = word_a.lower()\n",
        "    word_b = word_b.lower()\n",
        "    word_c = word_c.lower()\n",
        "    \n",
        "    (e_a, e_b, e_c) = (word_vectors[word2index[word_a]], \n",
        "                       word_vectors[word2index[word_b]], \n",
        "                       word_vectors[word2index[word_c]])\n",
        "    \n",
        "    \n",
        "    max_cosine_sim = -999\n",
        "    best_word = None\n",
        "    \n",
        "    for (w, i) in word2index.items():\n",
        "        if w in [word_a, word_b, word_c]:\n",
        "            continue\n",
        "        cosine_sim = cosine_distance(e_b - e_a, word_vectors[i] - e_c)\n",
        "        \n",
        "        if cosine_sim > max_cosine_sim:\n",
        "            max_cosine_sim = cosine_sim\n",
        "            best_word = w\n",
        "            \n",
        "    return best_word\n",
        " \n",
        "#correct answer: France \n",
        "print(find_analogy('athens', 'greece', 'paris', wvecs, w2i))\n",
        "#correct answer: she\n",
        "print(find_analogy('stepson', 'stepdaughter', 'he', wvecs, w2i))\n",
        "#correct answer: Tuvalu\n",
        "print(find_analogy('bangkok', 'thailand', 'funafuti', wvecs, w2i))\n",
        "#correct answer: cars\n",
        "print(find_analogy('woman', 'women', 'car', wvecs, w2i))\n",
        "#write writes vanish vanishes\n",
        "print(find_analogy('write', 'writes', 'vanish', wvecs, w2i))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iFSUq5Bk5buq",
        "colab_type": "text"
      },
      "source": [
        "## Pre-trained contextualised representations (BERT)\n",
        "\n",
        "We will now use one of the recently most popular pre-trained representations: BERT (Bidirectional Encoder Representations from Transformers). BERT creates so called contextualised word embeddings. You will see the details of creating such representations later in the course. \n",
        "\n",
        "For now, you need to know that BERT produces word representations which are informed by immediate sentence context rather than the the whole corpus (like for `word2vec`). For example, we have two sentences: *The man went to the bank to open an account* and *The man was sitting by the bank of the river*. `word2vec` will give the same word embedding for the word *bank* in both sentences, while BERT embeddings for *bank* would be different for each sentence since the meaning of this word is different. We will see it later in the lab.\n",
        "\n",
        "We use the Pytorch interface for BERT by [Hugging Face](https://github.com/huggingface/transformers)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PP2xuYy6GstJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install transformers"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e0wAd9UnHEtu",
        "colab_type": "text"
      },
      "source": [
        "We will use the model released by Google. It is a small `base` model which ignores casing (*uncased*). BERT uses a specific preprocessing procedure, which is provided by the BertTokenizer.\n",
        "\n",
        "**Q. Why do we need to tokenise our inputs according to BERT rules?**\n",
        "\n",
        "*A. So that words in our inputs are recognised by the model and are matched to repective pre-trained representations.*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZjmYPHHw5Se-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from transformers import BertTokenizer, BertModel\n",
        "# Load pre-trained model tokenizer (vocabulary)\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C2UhMGlMq3Re",
        "colab_type": "text"
      },
      "source": [
        "BERT can take as input one or two sentences. In this tutorial, we will focus on one sentence as input. It should be surrounded by the special tokens `[CLS]` and `[SEP]`. You will understand the details of this pre-processing later in the course.\n",
        " "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i9_FERooxUQR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#input_sentence = corpus[7]\n",
        "# uncomment and test for another sentence\n",
        "input_sentence = 'After stealing money from the bank vault, the bank robber was seen fishing on the Mississippi river bank.'\n",
        "marked_input_sentence = \"[CLS] \" + input_sentence + \" [SEP]\"\n",
        "\n",
        "# tokenize our sentence with the BERT tokenizer\n",
        "tokenized_text = tokenizer.tokenize(marked_input_sentence)\n",
        "\n",
        "# print out the tokens\n",
        "print (tokenized_text)\n",
        "\n",
        "# map the token strings to their vocabulary indeces\n",
        "indexed_tokens = tokenizer.convert_tokens_to_ids(tokenized_text)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WUlXfydh1Uxv",
        "colab_type": "text"
      },
      "source": [
        "Note that some words have been split into subwords: for example, 'brownies' into 'brown' and '##ies'. BERT uses the WordPiece model. WordPiece is similar to BPE with the main difference that it forms a new subword by likelihood but not the next pair with highest frequency.\n",
        "\n",
        "As mentioned before, BERT can take up to two sentences in input. We need to specify which sentence our input belongs to. Single-sentence inputs only require a series of 1. If you want to process two sentences, we will assign each word in the first sentence a 0, and all tokens of the second sentence a 1."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "rcKPET9BHb-7",
        "colab": {}
      },
      "source": [
        "# mark each of the tokens as belonging to sentence 1\n",
        "segments_ids = [1] * len(tokenized_text)\n",
        "\n",
        "# convert our data to torch tensors and call the BERT model\n",
        "tokens_tensor = torch.tensor([indexed_tokens])\n",
        "segments_tensors = torch.tensor([segments_ids])\n",
        "\n",
        "print (segments_ids)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "npzWXeQATPs4",
        "colab_type": "text"
      },
      "source": [
        "We will now load the `bert-base-uncased`. We will see the definition of the model printed in the logging. The model is a deep neural network with 12 layers. The meaning of these layers again will be explained later in the course. \n",
        "\n",
        "`model.eval()` puts our model in evaluation mode as opposed to training mode. This switches off certain regularisation techniques that modify input.\n",
        " \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5Cx2eJi1a8R6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# load the pre-trained model\n",
        "model = BertModel.from_pretrained('bert-base-uncased')\n",
        "\n",
        "# put the model into evaluation mode\n",
        "model.eval()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a0A1Iu-p4FUL",
        "colab_type": "text"
      },
      "source": [
        "`torch.no_grad` tells PyTorch not to construct the compute graph during this forward pass. We are not running training and will not backpropagate."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ixhvaYkW_SfX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        " with torch.no_grad():\n",
        "\n",
        "    outputs = model(tokens_tensor, segments_tensors)\n",
        "    print(outputs[1].size())\n",
        "    # we take our first example\n",
        "    word_vectors = outputs[0]\n",
        "    print(word_vectors.size())\n",
        "    # we do not need the batch dimension here since we have only one sentence\n",
        "    word_vectors = torch.squeeze(word_vectors, dim=0)\n",
        "    print(word_vectors.size())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "py8SOCIU4PM1",
        "colab_type": "text"
      },
      "source": [
        "Print out indices of the three instances of the word *bank* in the example sentence."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Tfr_kJRa5WGg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for i, token_str in enumerate(tokenized_text):\n",
        "  print (i, token_str)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "64XVBt0cjBJa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print('First 5 vector values for each instance of \"bank\".')\n",
        "print('')\n",
        "print(\"bank vault   \", str(word_vectors[6]))\n",
        "print(\"bank robber  \", str(word_vectors[10]))\n",
        "print(\"river bank   \", str(word_vectors[19]))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eA8ZiDNTolVx",
        "colab_type": "text"
      },
      "source": [
        "**Q. Calculate the cosine similarity between the word *bank* in (a) *bank robber* vs *river bank* (different meanings); (b) *bank robber* vs *bank vault* (same meaning). What are your conclusions?**\n",
        "\n",
        "*A. Similarity between instances of *bank* with similar meaning is higher.*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FM-q-txvjdPa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# \"bank robber\" vs \"river bank\" (different meanings)\n",
        "diff_bank = cosine_distance(np.array(word_vectors[10]), np.array(word_vectors[19]))\n",
        "\n",
        "# in \"bank robber\" vs \"bank vault\" (same meaning)\n",
        "same_bank = cosine_distance(np.array(word_vectors[10]), np.array(word_vectors[6]))\n",
        "\n",
        "print('Vector similarity for  *similar*  meanings:  %.2f' % same_bank)\n",
        "print('Vector similarity for *different* meanings:  %.2f' % diff_bank)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "woCVzfob-wYB",
        "colab_type": "text"
      },
      "source": [
        "## Advanced: Compositionality\n",
        "\n",
        "**Q. Given access to only word representations, how can we build representations for phrases and sentences?**\n",
        "\n",
        "*A. averaging, element-wise sum, concatenation of the embeddings for composing words*\n",
        "\n",
        "**Q. Compute the sentence vectors for the two sentences below. What is the problem with this way to create sentence representations?**\n",
        "\n",
        "*A. The word order is ignored.*\n",
        "\n",
        "**Q. Would averaging BERT embeddings help?**\n",
        "\n",
        "*A. Yes, the similarity is slightly lower.*\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZRQJmn7SlT3Y",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sent1 = 'it was really not good , on the opposite quite bad .'\n",
        "sent2 = 'it was really not bad , on the opposite quite good .'\n",
        "\n",
        "def get_sent_embedding(sent):\n",
        "  sent_embedding = []\n",
        "  for word in sent.split(' '):\n",
        "    sent_embedding.append(wvecs[w2i[word]])\n",
        "  sent_embedding = np.mean(np.array(sent_embedding), axis=0)\n",
        "  return sent_embedding\n",
        "\n",
        "sent1_embedding = get_sent_embedding(sent1)\n",
        "sent2_embedding = get_sent_embedding(sent2)\n",
        "\n",
        "print(cosine_distance(sent1_embedding, sent2_embedding))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f8-6RrsFlxEZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def preprocess_for_bert(sent):\n",
        "  marked_input_sentence = \"[CLS] \" + sent + \" [SEP]\"\n",
        "  tokenized_text = tokenizer.tokenize(marked_input_sentence)\n",
        "  indexed_tokens = tokenizer.convert_tokens_to_ids(tokenized_text)\n",
        "  segments_ids = [1] * len(tokenized_text)\n",
        "  tokens_tensor = torch.tensor([indexed_tokens])\n",
        "  segments_tensors = torch.tensor([segments_ids])\n",
        "  return tokens_tensor, segments_tensors\n",
        "\n",
        "sent1_tokens, sent1_segments = preprocess_for_bert(sent1)\n",
        "sent2_tokens, sent2_segments = preprocess_for_bert(sent2)\n",
        "\n",
        "\n",
        "with torch.no_grad():\n",
        "\n",
        "    outputs_sent1 = model(sent1_tokens, sent1_segments)\n",
        "    word_vectors_sent1 = outputs_sent1[0]\n",
        "    word_vectors_sent1 = torch.squeeze(word_vectors_sent1, dim=0)\n",
        "    sent1_embedding = word_vectors_sent1.mean(dim=0)\n",
        "    sent1_embedding = sent1_embedding.detach().cpu().numpy()\n",
        "\n",
        "    outputs_sent2 = model(sent2_tokens, sent2_segments)\n",
        "    word_vectors_sent2 = outputs_sent2[0]\n",
        "    word_vectors_sent2 = torch.squeeze(word_vectors_sent2, dim=0)\n",
        "    sent2_embedding = word_vectors_sent2.mean(dim=0)\n",
        "    sent2_embedding = sent2_embedding.detach().cpu().numpy()\n",
        "\n",
        "\n",
        "print(cosine_distance(sent1_embedding, sent2_embedding))\n",
        " \n",
        "  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X9XrVxOZlF9h",
        "colab_type": "text"
      },
      "source": [
        "# Assignment 1: Training with negative sampling\n",
        "\n",
        "Fill in the gaps in the code below for training the skipgram model with negative sampling. Train the model using the following corpus:  https://raw.githubusercontent.com/pytorch/examples/master/word_language_model/data/wikitext-2/train.txt. Use can use the GPU Colab Runtime. Submit the code and a short report (150 words) answering the following questions:  \n",
        "\n",
        "1. How did you preprocess your dataset and why?\n",
        "2. How did you pick negative examples? How many of them did you choose? Why did you decide to do so?\n",
        "3. How did you compute the final loss? \n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NVvwQ7LJr2T2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# The two weight matrices:\n",
        "W1 = torch.randn(embedding_dims, vocabulary_size, requires_grad=True)\n",
        "W2 = torch.randn(embedding_dims, vocabulary_size, requires_grad=True)\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    epoch_loss = 0\n",
        "    for data, target in idx_pairs:\n",
        "        x_var = Variable(look_up_table(data)).float() \n",
        "        \n",
        "        y_pos = Variable(torch.from_numpy(np.array([target])).long())\n",
        "        y_pos_var = Variable(look_up_table(target)).float()\n",
        "\n",
        "        # TO DO: pick a negative sample\n",
        "        neg_sample = \n",
        "        \n",
        "        #TO DO: use the look up table\n",
        "         \n",
        "        x_emb = torch.matmul(W1, x_var) \n",
        "        y_pos_emb = torch.matmul(W2, y_pos_var)\n",
        "        y_neg_emb = torch.matmul(W2, y_neg_var)\n",
        "        \n",
        "        # get positive sample score\n",
        "        pos_loss = F.logsigmoid(torch.matmul(x_emb, y_pos_emb))\n",
        "        \n",
        "        # get negsample score\n",
        "        neg_loss = F.logsigmoid(-1 * torch.matmul(x_emb, y_neg_emb))\n",
        "        \n",
        "        # TO DO: compute loss\n",
        "        loss =  \n",
        "        epoch_loss += loss.item()\n",
        "        \n",
        "        # propagate the error\n",
        "        loss.backward()\n",
        "        \n",
        "        # gradient descent\n",
        "        W1.data -= learning_rate * W1.grad.data\n",
        "        W2.data -= learning_rate * W2.grad.data\n",
        "\n",
        "        # zero out gradient accumulation\n",
        "        W1.grad.data.zero_()\n",
        "        W2.grad.data.zero_()\n",
        "        \n",
        "    if epoch % 10 == 0:    \n",
        "        print(f'Loss at epoch {epoch}: {epoch_loss/len(idx_pairs)}')"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}